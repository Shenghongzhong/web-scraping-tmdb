{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhAPlKWNn8aj"
   },
   "outputs": [],
   "source": [
    "# Jovian Commit Essentials\n",
    "# Please retain and execute this cell without modifying the contents for `jovian.commit` to work\n",
    "!pip install jovian --upgrade -q\n",
    "import jovian\n",
    "jovian.set_project('web-scraping-tmdb')\n",
    "jovian.set_colab_id('1M40pNjjU7MgWi55Yqk8PcGyGZdVid0h7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYEn3ZJXU2JM"
   },
   "source": [
    "# Web Scraping Popular Television Shows on TMDB ([themoviedb.org](https://www.themoviedb.org))\n",
    "\n",
    "data source: TMBd website ([https://www.themoviedb.org](https://www.themoviedb.org))\n",
    "\n",
    ">A disclaimer before beginning, many websites restrict or outright bar scraping of data from their pages. Users may be subject to legal ramifications depending on where and how they attempt to scrape information. Many sites have a devoted page to noting restrictions on data scraping at **www.[site].com/robots.txt**. Be extremely careful if looking at sites that house user data — places like facebook, linkedin, even craigslist, do not take kindly to data being scraped from their pages. When in doubt, please contact with teams at sites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xASA6FpjVXZr"
   },
   "source": [
    "![](https://i.imgur.com/qi5y4LB.png)\n",
    "\n",
    "The Movie Database (TMDb) is a community-driven website about movies and television shows database. The community has added every piece of data since 2008. Users can search for their desired topics and discover what they like after browsing a large amount of data. Users can also contribute to the TMDb community by giving reviews and their scores to certain shows for the benefits of the community. In summary, TMDb is an excellent website for someone like me who wanted to practice web scraping skills.\n",
    "\n",
    "### Project motivation\n",
    "For the purpose of this project, we will retrieve information from the page of **’Popular TV Shows’** using _web scraping_: a process of extracting information from a website programmatically. Web scraping isn’t magic, and yet some readers may grab information on a daily basis. For example, a recent graduate may copy and paste information about companies they applied for into a spreadsheet for job application management.\n",
    "\n",
    "#### Project goals\n",
    "\n",
    "The project goal is to build a web scraper that withdraws all desirable information and assemble them into a single CSV. The format of the output CSV file is shown below:\n",
    "\n",
    "|#|movie_title|released_date|score|image_link|detail-page|\n",
    "|-|-----------|-------------|-----|----------|----|\n",
    "|1|The Falcon and the Winter Solider| 19 Mar 2021| 78%|..|...|\n",
    "|2|The Good Doctor| 25 Sep 2017| 86%| ...|\n",
    "....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xYkSodNY8Ra"
   },
   "source": [
    "## Project steps\n",
    "Here is an outline of the steps we'll follow.\n",
    "\n",
    "1. Doanload the webpage using `requests`\n",
    "2. Parse the HTML source code using `BeautifulSoup` library\n",
    "3. Building the scraper components\n",
    "4. Complie extracted information into Python list and dictionaries\n",
    "5. Write information to CSV files\n",
    "6. Extract and combine data from multiple pages\n",
    "7. Future work and references\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG0l8f6tRDYb"
   },
   "source": [
    "### How to run the code\n",
    "\n",
    "This tutorial is an executable [Jupyter notebook](https://jupyter.org) hosted on [Jovian](https://www.jovian.ai). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
    "\n",
    "#### Option 1: Running using free online resources (1-click, recommended)\n",
    "\n",
    "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Binder**. You can also select \"Run on Colab\" or \"Run on Kaggle\", but you'll need to create an account on [Google Colab](https://colab.research.google.com) or [Kaggle](https://kaggle.com) to use these platforms.\n",
    "\n",
    "\n",
    "#### Option 2: Running on your computer locally\n",
    "\n",
    "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions.\n",
    "\n",
    ">  **Jupyter Notebooks**: This tutorial is a [Jupyter notebook](https://jupyter.org) - a document made of _cells_. Each cell can contain code written in Python or explanations in plain English. You can execute code cells and view the results, e.g., numbers, messages, graphs, tables, files, etc., instantly within the notebook. Jupyter is a powerful platform for experimentation and analysis. Don't be afraid to mess around with the code & break things - you'll learn a lot by encountering and fixing errors. You can use the \"Kernel > Restart & Clear Output\" menu option to clear all outputs and start again from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "KDRTSPLZU2JN",
    "outputId": "11786e5f-beff-439a-f5ce-c31bcc1c5c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "[jovian] Capturing environment..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/shenghongzhong/210424-project001-web-scraping-tmbd\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/shenghongzhong/210424-project001-web-scraping-tmbd'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install jovian --upgrade --quiet\n",
    "import jovian\n",
    "jovian.commit(project=\"210424-project001-web-scraping-tmbd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tYUKwOfY91d"
   },
   "source": [
    "# Web Scraping TV shows from TMBd (themoviedb.org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMpM-ptbh6Vg"
   },
   "source": [
    "## Download the webpage using `requests`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBI77PZ7Z0Bw"
   },
   "source": [
    "### **Requests**\n",
    "\n",
    "### **World Wide Web**\n",
    "\n",
    "Before we explain what `requests` library is, we have to ask a question about **WHY we need to use requests**. This leads to the origin of the World Wide Web. \n",
    "\n",
    "Since 1989, [Tim Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee) proposed the concept of the [World Wide Web](http://info.cern.ch/hypertext/WWW/TheProject.html) as an open platform where users can share information quickly and locate it from anywhere in the world. This enables all scientists to continue their research without going back to their home countries at that time.\n",
    "\n",
    "### **Three key components**\n",
    "\n",
    "![](https://i.imgur.com/eAmOdX0.png)\n",
    "\n",
    "### **HTML**\n",
    "\n",
    "We can break down the web into three key things. The first one is the **HyperText Markup Language**, short for HTML. It’s the standard markup language for documents designed to be displayed in web browsers. What HTML does is to present content, just like a World document which describes paragraphs of texts, images, tables of data.\n",
    "\n",
    "### **URL**\n",
    "\n",
    "The second one is the URL. It stands for **Uniform Resource Locator**, which is what you would enter into your address bar in the Chrome browser every day. What a URL does is to take you to the same page every single time. It’s approximately what your phone number does. If someone phones your telephone number, they’re always going to contact you.\n",
    "\n",
    "### **HTTP**\n",
    "\n",
    "Last but at least, HTTP is a part of the web. It’s an invisible layer underneath the surface that is doing the communication with a server and your browser. For example, when you log into Twitter, you’ll type in your username and passwords. Then, you hit the button “Submit” and those details would be sent using an HTTP request to Twitter servers. Next, the servers will send **an HTTP response** after processing if the username and passwords are correct.\n",
    "\n",
    "In a nutshell, **HTTP** is the fundamental way that websites (Your browser) **communicate** with **servers which are just giant computers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIujogcFotVO"
   },
   "source": [
    "### **What is `requests`**\n",
    "\n",
    "\n",
    "Requests is a Python HTTP library that allows us to send HTTP requests to servers of websites, instead of using browsers to communicate the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsbmDR1IBYRw"
   },
   "source": [
    "We use `pip`, a package-management system, to install and manage softwares. Since the platform we selected is **Google colab**, we would have to type a line of code `!pip install` to install `requests`. You will see lots codes of `!pip` when installing other packages.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5A4X8B5BZz5"
   },
   "source": [
    "When we attempt to use some prewritten functions from a certain library, we would use the `import` statement. e.g. When we would have to type `import requests` after installation, we are able to use any function from `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8Agmq1iZvL2",
    "outputId": "70274d81-a9e7-43d9-d241-6d4508329da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |█████▍                          | 10kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 20kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 30kB 27.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 40kB 21.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 51kB 15.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install requests --upgrade --quiet\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnx-LKOMqi4x"
   },
   "source": [
    "### **URL structure**\n",
    "\n",
    "Since we focused on the popular TV shows, the URL we're landing on is `https://themoviedv.org/tv`. Having analyzed the URL structure, I found out a trick that you can access to the specific page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWX1EBLEqfm_"
   },
   "source": [
    "![](https://i.imgur.com/N6BpDf7.png)\n",
    "\n",
    "![](https://i.imgur.com/PuSDSHx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOk6RGTHrMt4"
   },
   "source": [
    "#### **requests.get()**\n",
    "\n",
    "In order to **download a web page**, we use `requests.get()` to **send the HTTP request** to the **TMBd server** and what the function returns is a **response object**, which is **the HTTP response**. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T1c9cFmCFfb"
   },
   "source": [
    "Since **a URL** can always lead us to a certain page and we know how TMDb structured their website,  I assigned the variable `base_url` to a value of `https://themoviedb.org` and `tmbd_url` to a value of `https://themoviedb.org/tv?page=5`. To be explicit, I named the variable `response` to be assigned to the HTTP response containing page contents and other information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guiuDxDYCHBF"
   },
   "source": [
    "\n",
    "Later on, I intended to **design a function** that asked people to input any page they wanted and **pass the input value** to replace the **number `5`**. Using this design thinking, people can achieve the outcome for either one page of data or **X pages of data** at TMDb. \n",
    "\n",
    "The reason why I said **\"design thinking\"** is that we are supposed to have **a product mindset**. The technology is used for human!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5uS_ewrriFM",
    "outputId": "ea557e83-2ac9-418a-a4c1-1981aaa5d11e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://themoviedb.org/tv?page=5\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://themoviedb.org'\n",
    "tmbd_url = base_url + '/tv?page=' + '5'\n",
    "print(tmbd_url)\n",
    "response = requests.get(tmbd_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezMCP4UtsGnT"
   },
   "source": [
    "### **Status code**\n",
    "\n",
    "Another thing here is that we have to **check** if we succesfully send the HTTP request and get a HTTP response back on purpose. This is because we're NOT using browsers which we can't get **the feedback** straightforwardly if we didn't send HTTP requests successfully.\n",
    "\n",
    "In general, the method to check out if the server sended a HTTP response back is the **status code**. In `requests` library, `requests.get` returns a response object, which containing the page contents and the information about status code indicating if the HTTP request was successful. Learn more about HTTP status codes here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status.\n",
    "\n",
    "\n",
    "If the request was successful, `response.status_code` is set to a value between **200 and 299**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbjsihq1DHgG"
   },
   "source": [
    "The name of variable is  `response`  but the variable understood by Python is **a response object**. Don't be confused!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBxSsQCRZEHQ",
    "outputId": "963b07a2-f26a-40eb-8e24-9f89af96323e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqkGqpaTZBHc",
    "outputId": "28802004-eb5c-4711-ec29-c215cbfac06c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uDW9uM8DfIJ"
   },
   "source": [
    "*If you see the status_code is 200, then it means `requests.get(thmbd_url)` was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lalDCgfJr07q",
    "outputId": "e2838c47-c42e-434f-af19-742faa91f785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RtYWJwKsjir"
   },
   "source": [
    "The HTTP response contains HTML that is ready to be displayed in browser. Here we can use `response.text` to retrive the HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKSIOrMItHTf"
   },
   "source": [
    "As a result, we have 220K characters! Let's use `page_contents[:1000]` to preview what we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N67etq8StDt5",
    "outputId": "04c5ca95-6ac7-4328-fd2f-4f18b08be5df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186253"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents = response.text\n",
    "len(page_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pkX1fACD2iE"
   },
   "source": [
    "What the HTML syntax looks like if we preview the 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "56sf0Ok_tWya",
    "outputId": "0ff177dc-29e3-4157-ea2a-b99550c8f24f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\" class=\"no-js\">\\n  <head>\\n    <title>Popular TV Shows &#8212; The Movie Database (TMDb)</title>\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\\n    <meta http-equiv=\"cleartype\" content=\"on\">\\n    <meta charset=\"utf-8\">\\n    \\n    <meta name=\"keywords\" content=\"Movies, TV Shows, Streaming, Reviews, API, Actors, Actresses, Photos, User Ratings, Synopsis, Trailers, Teasers, Credits, Cast\">\\n    <meta name=\"mobile-web-app-capable\" content=\"yes\">\\n    <meta name=\"apple-mobile-web-app-capable\" content=\"yes\">\\n    <meta name=\"HandheldFriendly\" content=\"True\">\\n    <meta name=\"MobileOptimized\" content=\"320\">\\n    \\n    <meta name=\"viewport\" content=\"width=1120\">\\n    \\n    <meta name=\"msapplication-TileImage\" content=\"/assets/2/v4/icons/mstile-144x144-30e7905a8315a080978ad6aeb71c69222b72c2f75d26dab1224173a96fecc962.png\">\\n<meta name=\"msapplication-TileColor\" content=\"#032541\">\\n<meta name=\"theme-color\" content=\"#032541\">\\n<link rel=\"apple-touch-icon\" sizes=\"180x180\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BhTPuqXZo9d"
   },
   "source": [
    "- What you see above is the source code of the web page. It written in a language called HTML. \n",
    "- It defines and display the content and structure of the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ba3GkeH-to9c"
   },
   "source": [
    "Let's save the text into a file with `open` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCZFjQPltlqK"
   },
   "outputs": [],
   "source": [
    "with open('tmbd_popular_tv.html',\"w\") as f:\n",
    "  f.write(page_contents)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq2ZtaSzZ4cZ"
   },
   "source": [
    "Just in case you would like to view how HTML looks like, you can now view the file by clicking the folder-like icon using the side bar on your left. \n",
    "\n",
    "![](https://i.imgur.com/pCzoncM.png)\n",
    "\n",
    "Here's what you'll see when you open the HTML file on browser:\n",
    "\n",
    "![](https://i.imgur.com/nNyUaMx.png)\n",
    "\n",
    "\n",
    "Here's what you'll see when you open the HTML file on text  editor:\n",
    "\n",
    "![](https://i.imgur.com/XcKcSAA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCR5f_uuikhA"
   },
   "source": [
    "### Summary\n",
    "\n",
    "- We know the origin of world wide web and basics about 3 key components ( HTML, URL and HTTP)\n",
    "\n",
    "- We know how to use `requests.get`  to get the page contents of a URL and return a response object.\n",
    "\n",
    "- We know how to check if a request is successful by using `response.status_code`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJFx7uWejwrn"
   },
   "source": [
    "#### Let's wrap the codes up into a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPcaMjoilJGE"
   },
   "outputs": [],
   "source": [
    "def get_page(page_number):\n",
    "    \"\"\"Get the number of web page containing all the content for TV shows and retun a BeautifulSoup document\"\"\"\n",
    "    page_url = 'https://www.themoviedb.org/tv' + '?page=' + str(page_number)\n",
    "    response = requests.get(page_url)\n",
    "    #check the status\n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception ('Failed to fetch web page' + page_url)\n",
    "    #return a BeautifulSoup object\n",
    "    return BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fatFHiP78t7"
   },
   "source": [
    "# Parse the HTML source code using Beautiful Soup library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs_VsM1nuDMs"
   },
   "source": [
    "## What is Beautiful Soup?\n",
    "\n",
    "You might wonder what `BeautifulSoup(response.text)` is as you look at each line of codes for my last helper function `get_page()`. It was a hint for this section. \n",
    "\n",
    "Beautiful Soup is **a Python package** for **parsing HTML and XML documents**. Beautiful Soup enables us to get data out of sequences of characters. It creates a parse tree for parsed pages that can be used to extract data from HTML. It's a handy tool when it comes to web scraping. You can read more on their documentation site. https://www.crummy.com/software/BeautifulSoup/bs4/doc/#getting-help\n",
    "\n",
    "To extract information from the HTML source code of a webpage programmatically, we can use the Beautiful Soup library. Let's install the library and import **the BeautifulSoup class** from **the bs4 module.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvsQ4QCquC7_"
   },
   "outputs": [],
   "source": [
    "#Install Beautiful Soup package\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Moo0C2ZEeNIw"
   },
   "outputs": [],
   "source": [
    "# from...import meaning you don't need to type bs4.BeautifulSoup everytime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UM28ljyWZHFI"
   },
   "outputs": [],
   "source": [
    "# Open the file\n",
    "with open('tmbd_popular_tv.html',\"r\") as f:\n",
    "  tmbd = f.read()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqSZf1mJEtpg"
   },
   "source": [
    "You can either use `open` statement to open the html file we just saved or to use `response.text` to get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEUqVDEDZHCm",
    "outputId": "cf819e60-f9e7-45d2-b5f1-dcec85cfb70a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it's the same\n",
    "tmbd == response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXfD4SpswFe9"
   },
   "source": [
    "### Inspecting the HTML source code of a web page\n",
    "\n",
    "![](https://i.imgur.com/zF02MhD.png)\n",
    "\n",
    "#### HTML basics\n",
    "\n",
    "Before we dive into how to inspect HTML, we should know the basic knowledge about HTML.\n",
    "\n",
    "In the late 1980s, a British scientist Tim Bereners-Lee invented HTML, which stands for HyperText Markup Language, while working at a CERN laboratory in Switzerland. He didn't want to make the page contents displayed on the web just regular text files. In order to increase communication efficiency and liberate people's creativity, he tried to let authors have the ability to define each part of the texts. Hence, the content displayed on web pages is written in HTML.\n",
    "\n",
    "In Beautiful Soup library, we can specify `html.parser` to ask Python to read components of the page, instead of reading it as a long string. \n",
    "\n",
    "We can use `<title>` tag as an example to demonstrate what **`tag'** is in HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoiJDkqyZHAb"
   },
   "outputs": [],
   "source": [
    "document = BeautifulSoup(tmbd,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlahWDol5rUT"
   },
   "source": [
    "#### The `<title>` tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgzg0xdHy9if",
    "outputId": "eec21514-ecfd-4637-b011-5b686cd5d90d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Popular TV Shows — The Movie Database (TMDb)</title>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPm6s5soFPPv"
   },
   "source": [
    "![](https://i.imgur.com/5Lx3dbu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1NM21sM6J5I"
   },
   "source": [
    "What we can do with **a BeautifulSoup object** is to get **a specifc types of a tag in HTML** by calling the name of a tag, as shown in code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1V02F8d6Wl5"
   },
   "source": [
    "To be explicit, let's call the variable `'title_tag'`, and we can get the text inside the title tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "T-wJEdld5w9C",
    "outputId": "717126e4-90bc-48ab-c751-2e0551ca64b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Popular TV Shows — The Movie Database (TMDb)'"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag = document.title\n",
    "title_tag.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPkwxRLzzY6c"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#### **An HTML tag comprises of three parts:**\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`.\n",
    "\n",
    "\n",
    "### Common tags and attributes\n",
    "\n",
    "#### **Tags in HTML**\n",
    "\n",
    "There are around 100 types of HTML tags but on a day to day basis, around 15 to 20 of them are the most common use, such as `<div>` tag, `<p>` tag, `<section>` tag, `<img>` tag, `<a>` tags.\n",
    "\n",
    "![](https://i.imgur.com/sL4gp0l.png)\n",
    "\n",
    "\n",
    "Of many tags, I wanted to highlight **`<a>` tag**, which  can contain attributes such as `href` (hyperlink reference), because `<a>` tag allows users to click and they would be directed to another site. That's why the name of `<a>` tag is  **anchor**.\n",
    "\n",
    "#### **Attributes**\n",
    "\n",
    "Each tag supports several attributes. Following are some common attributes used to modify the behavior of tags\n",
    "\n",
    "* `id`\n",
    "* `style`\n",
    "* `class`\n",
    "* `href` (used with `<a>`)\n",
    "* `src` (used with `<img>`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4k05q8MZHpA"
   },
   "source": [
    "## Building the scraper components\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf5TBXPTvF0z"
   },
   "source": [
    "In this section, we are starting to build pieces of components for our scraper to extract movie titles,released date and detail URL. As mentioned eariler, what the outcome we want is a CSV file containing as follows:\n",
    "\n",
    "|#|movie_title|released_date|score|image_link|detail_page|\n",
    "|-|-----------|-------------|-----|----------|----|\n",
    "|1|The Falcon and the Winter Solider| 19 Mar 2021| 78%|..|...|\n",
    "|2|The Good Doctor| 25 Sep 2017| 86%| ...|\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNOjZVWvZHmk"
   },
   "source": [
    "\n",
    "### Inspecting HTML in the Browser\n",
    "\n",
    "To view the **source code** of any webpage right within **your browser**, you can **right click** anywhere on a page and **select** the **\"Inspect\"** option. You access the **\"Developer Tools\"** mode, where you can see the source code as **a tree**. You can expand and collapse various nodes and find the source code for a specific portion of the page\n",
    "\n",
    "![](https://i.imgur.com/goG29IX.png)\n",
    "\n",
    "\n",
    "As shown in the photo above, I've cursored over one of the TV programs to display how the entire content was presented. I found out the data on each page *is* held within a `<div>` tag with the attribute `class=\"page_wrapper\"`. Its children tags is another `<div>` tag including the `class=\"content\"`. That's a good sign. We will not need to know every attribute of every tag to extract our information, but it is helpful to analyze the structure of HTML source code. \n",
    "\n",
    "Since I've pulled a single page and return to a BeautifulSoup object, we can start to use some function from Beautiful Soup library to withdraw the piece of information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnCpy5upZIQV"
   },
   "outputs": [],
   "source": [
    "# Find the <div class=\"page_wrapper\"\n",
    "page_wrapper = document.find('div',class_='page_wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70dsAvFDZO2a"
   },
   "outputs": [],
   "source": [
    "#Pull all <div class=\"content\"> under the parent tag <div class=\"page_wrapper\">\n",
    "content_tags = page_wrapper.find_all('div',class_='content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXAqVjf6KNtk"
   },
   "source": [
    "By looking at the page, there must be 20 tv programs on one page. Therefore, each function I write to withdraw a piece of information should yield 20 different items. \n",
    "\n",
    "If my output provides **fewer** than the total number of **20**, then it indicates something went **WRONG** and time to refer back to the page itself to debug the code.\n",
    "\n",
    "Also, at the end of section, I wrote a for loop for each. It can help me see some unknown problems such as there is no data for images or no released date for a certain TV program. This happens in the real life. Some TV programs might announce \"We're going to make it happen!\" but it didn't as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtPlO-iBHMCy"
   },
   "source": [
    "On the page, we know the total items is 20 and I put these values into a variable called `content_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnIN2yVzZOz6",
    "outputId": "d239e980-a1da-4d07-aaca-daee8cc533ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the total number is 20\n",
    "len(content_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki0aBDl7L1DN",
    "outputId": "ff7f7958-a664-4e8e-baf6-c0243c88a946"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"content\">\n",
       " <div class=\"consensus tight\">\n",
       " <div class=\"outer_ring\">\n",
       " <div class=\"user_score_chart 5aea223992514172d9001199\" data-bar-color=\"#21d07a\" data-percent=\"84.0\" data-track-color=\"#204529\">\n",
       " <div class=\"percent\">\n",
       " <span class=\"icon icon-r84\"></span>\n",
       " </div>\n",
       " </div>\n",
       " </div>\n",
       " </div>\n",
       " <h2><a href=\"/tv/79242\" title=\"Chilling Adventures of Sabrina\">Chilling Adventures of Sabrina</a></h2>\n",
       " <p>Oct 26, 2018</p>\n",
       " </div>, <div class=\"content\">\n",
       " <div class=\"consensus tight\">\n",
       " <div class=\"outer_ring\">\n",
       " <div class=\"user_score_chart 5256cfba19c2956ff60a01e4\" data-bar-color=\"#21d07a\" data-percent=\"77.0\" data-track-color=\"#204529\">\n",
       " <div class=\"percent\">\n",
       " <span class=\"icon icon-r77\"></span>\n",
       " </div>\n",
       " </div>\n",
       " </div>\n",
       " </div>\n",
       " <h2><a href=\"/tv/1418\" title=\"The Big Bang Theory\">The Big Bang Theory</a></h2>\n",
       " <p>Sep 24, 2007</p>\n",
       " </div>, <div class=\"content\">\n",
       " <div class=\"consensus tight\">\n",
       " <div class=\"outer_ring\">\n",
       " <div class=\"user_score_chart 52589cbd760ee3466161068e\" data-bar-color=\"#21d07a\" data-percent=\"87.0\" data-track-color=\"#204529\">\n",
       " <div class=\"percent\">\n",
       " <span class=\"icon icon-r87\"></span>\n",
       " </div>\n",
       " </div>\n",
       " </div>\n",
       " </div>\n",
       " <h2><a href=\"/tv/31132\" title=\"Regular Show\">Regular Show</a></h2>\n",
       " <p>Sep 06, 2010</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview top 3 content tag\n",
    "content_tags[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aZKjp6FMPt9"
   },
   "source": [
    "### 1. Movie titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5v_zw6-MhkW"
   },
   "source": [
    "As noted above,the entire tv program is nested under `<div class=\"content\">` tags. Having looked it into details, I could see that movie titles are listed under `<a>` tags with the attribute `title= \"[THE MOVIE TITLES]\"`, Here is the texts for `<a>` tags are the same as values of  `title` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBmq35-1Ha0d"
   },
   "source": [
    "Let's preview the first TV program we captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfSMbsWUMYYu",
    "outputId": "baedb29d-80cf-4ae4-bd58-816930772545"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content\">\n",
       "<div class=\"consensus tight\">\n",
       "<div class=\"outer_ring\">\n",
       "<div class=\"user_score_chart 5aea223992514172d9001199\" data-bar-color=\"#21d07a\" data-percent=\"84.0\" data-track-color=\"#204529\">\n",
       "<div class=\"percent\">\n",
       "<span class=\"icon icon-r84\"></span>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<h2><a href=\"/tv/79242\" title=\"Chilling Adventures of Sabrina\">Chilling Adventures of Sabrina</a></h2>\n",
       "<p>Oct 26, 2018</p>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To view what we have re content tag under the id page_1\n",
    "first_content_tag = content_tags[0]\n",
    "first_content_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5aQLBwKHf15"
   },
   "source": [
    "Having checked the first TV program in our collected data, we found the movie title is housed in the `<a>` tag. So we can use the method `Beautiful.find()` to find the same type of `<a>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MueAQ2jN23d",
    "outputId": "e9c288f1-10a8-4c68-9ace-9f715c880b7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/tv/79242\" title=\"Chilling Adventures of Sabrina\">Chilling Adventures of Sabrina</a>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the first title via a tag\n",
    "a_tag = first_content_tag.find('a')\n",
    "a_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pfvRHD_qN7fy",
    "outputId": "cc4cb35e-167e-47f3-f988-39d462a56c34"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Chilling Adventures of Sabrina'"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get text inside a tag\n",
    "a_tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JndVQ_vN_eF",
    "outputId": "b3f6431c-1243-409c-e06d-dcd10de5fed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of tv program on the page is 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Chilling Adventures of Sabrina',\n",
       " 'The Big Bang Theory',\n",
       " 'Regular Show',\n",
       " 'Lady, la vendedora de rosas',\n",
       " 'Arrow',\n",
       " 'Tokyo Revengers',\n",
       " 'Once',\n",
       " 'Rebelde',\n",
       " 'Money Heist',\n",
       " 'New Amsterdam',\n",
       " 'Young Sheldon',\n",
       " 'Teresa',\n",
       " 'The Nevers',\n",
       " 'Batwoman',\n",
       " 'Friends',\n",
       " 'Rebelde Way',\n",
       " 'SEAL Team',\n",
       " 'American Gods',\n",
       " 'Endless Love',\n",
       " 'Chicago P.D.']"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a for loop to see\n",
    "content_tags = page_wrapper.find_all('div',class_=\"content\")\n",
    "movie_title_list = [content_tag.find('a').text for content_tag in content_tags ]\n",
    "print(\"the total number of tv program on the page is {}\".format(len(movie_title_list)))\n",
    "movie_title_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0A9kQ36O9v4"
   },
   "source": [
    "### 2. Released date\n",
    "\n",
    "The data about the release date is actually living in the same tag `<p>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xv-I0AFzPboU",
    "outputId": "0ac2b3b1-3e3c-4e5f-f1e2-44663f11c0ae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Oct 26, 2018'"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the first released date via p tag\n",
    "p_tag = content_tags[0].find('p')\n",
    "p_tag.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAmsuRJ9PnM4"
   },
   "source": [
    "You might not notice here is a small **issue** with the date. Since our output is a CSV file, and CSV stands for Comma-separated values. It is a delimited text file that uses **a comma** to **separate values**. Each line of the file is a data record.\n",
    "\n",
    "In this case, we will mess up if we don't think of ways to clean up the data. Fortunately, we have **a module** called `datetime` to help us. Also, I developed if/else statement to avoid some tv programs with NO released date. AKA, no value to be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNrXToa7PmiR",
    "outputId": "dab9fc60-ba4b-4ff8-d81c-0a749ac614b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "#To get a datetime object\n",
    "date = dt.datetime.strptime(content_tags[0].find('p').text, \"%b %d, %Y\")\n",
    "#To format the string with YYYY-MM-DD\n",
    "rel_date = '{}-{}-{}'.format(date.year,date.month,date.day)\n",
    "print(rel_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jL1YA3p9SwPY"
   },
   "outputs": [],
   "source": [
    "# Write a helper function to clean\n",
    "def convert_date(p_tag):\n",
    "  if p_tag =='':\n",
    "    return \"NotFound\"\n",
    "  elif p_tag is None:\n",
    "    return \"NotFound\"\n",
    "  else:\n",
    "    date = dt.datetime.strptime(p_tag, \"%b %d, %Y\")\n",
    "    released_date = '{}-{}-{}'.format(date.year,date.month,date.day)\n",
    "    return released_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WTnfpeBImeK"
   },
   "source": [
    "Let's write a for loop to see if we can capture the same type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0XjgzbZSYxh",
    "outputId": "889e27d1-02b6-449b-c77b-5a23b8d48cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of releasted dates on the page is 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2018-10-26',\n",
       " '2007-9-24',\n",
       " '2010-9-6',\n",
       " 'NotFound',\n",
       " '2012-10-10',\n",
       " '2021-4-11',\n",
       " '2017-6-19',\n",
       " '2004-10-4',\n",
       " '2017-5-2',\n",
       " '2018-9-25',\n",
       " '2017-9-25',\n",
       " '2010-8-2',\n",
       " '2021-4-11',\n",
       " '2019-10-6',\n",
       " '1994-9-22',\n",
       " '2002-5-27',\n",
       " '2017-9-27',\n",
       " '2017-4-30',\n",
       " '2015-10-14',\n",
       " '2014-1-8']"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "#Write a for loop to see all of dates on the same page\n",
    "content_tags = page_wrapper.find_all('div',class_=\"content\")\n",
    "p_tags = [content_tag.find('p') for content_tag in content_tags]\n",
    "released_date = [convert_date(p_tag.text) for p_tag in p_tags ]\n",
    "print(\"the total number of releasted dates on the page is {}\".format(len(released_date)))\n",
    "released_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgRUOZVnIsmN"
   },
   "source": [
    "Did you see we have a type of data called **Not found**. \n",
    "\n",
    "**Missing data is worse than no data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bCUG_jVQXp8"
   },
   "source": [
    "### 3. User score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX2KsEo4Q3FN"
   },
   "source": [
    "User score are located under the **`<span>` tags**. Initially, I tried getting the `<div>` tag using the attribute  `date-percent=\"79.0\"`. You can find  the value of a tag's attribute with `tag[\"attribute\"]`. But I decided to go with `<span>` tag simply because it makes my job easier. \n",
    "\n",
    "Be smart with you tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8exdEFb3RwTz",
    "outputId": "7f2edac9-5be3-4c1b-b976-d71c1adbb2dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"icon icon-r84\"></span>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the span tag \n",
    "span_tag = content_tags[0].find('span')\n",
    "span_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pzBfjNo9R13N",
    "outputId": "3e23f8c6-a0b0-4c52-cf41-878aed1b1dee"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'84'"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To find the value of the attribute name \"class\"\n",
    "user_score = span_tag['class'][1][-2:]\n",
    "user_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2MWvSLShBGB",
    "outputId": "a71cdfae-3c4c-41cc-8a8e-2171d4d10413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of user_score on the page is 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['84',\n",
       " '77',\n",
       " '87',\n",
       " '74',\n",
       " '66',\n",
       " '90',\n",
       " '88',\n",
       " '85',\n",
       " '83',\n",
       " '84',\n",
       " '80',\n",
       " '75',\n",
       " '87',\n",
       " '73',\n",
       " '84',\n",
       " '84',\n",
       " '78',\n",
       " '71',\n",
       " '77',\n",
       " '84']"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get a list of user score\n",
    "content_tags = page_wrapper.find_all('div',class_=\"content\")\n",
    "span_tags = [content_tag.find('span') for content_tag in content_tags]\n",
    "user_scores = [span_tag['class'][1][-2:] for span_tag in span_tags ]\n",
    "print(\"the total number of user_score on the page is {}\".format(len(user_scores)))\n",
    "user_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzGi5RVkUTvE"
   },
   "source": [
    "### 4. Image link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCA7bjMNXuZ4"
   },
   "source": [
    "Images were a bit tricky. Since it is housed in different types of tags under the `<div class=\"paper_wrapper\">`, we have to modify it. So we create a variable `a_tags_for_imgs` to capture all `<a>` tags containing the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OutkOm07W-hU",
    "outputId": "6e85a5a0-0a21-4ebd-ee75-1f1a7a6ee52b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"image\" href=\"/tv/79242\" title=\"Chilling Adventures of Sabrina\">\n",
       "<img alt=\"\" class=\"poster\" loading=\"lazy\" src=\"/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg\" srcset=\"/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg 1x, /t/p/w440_and_h660_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg 2x\"/>\n",
       "</a>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the first image for the website\n",
    "a_tags_for_img_tags = page_wrapper.find_all('a', class_='image')\n",
    "a_tags_for_img_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WITCroyOYJF_"
   },
   "source": [
    "Once we get the list of `<a>` tags, we can see `<img>` tags with the `class=\"image\"`. \n",
    "\n",
    "Let's get one image to see if we are successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5z8T2WqYLau",
    "outputId": "e7d3ff74-a9e5-4d93-977c-d0797a3def64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img alt=\"\" class=\"poster\" loading=\"lazy\" src=\"/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg\" srcset=\"/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg 1x, /t/p/w440_and_h660_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg 2x\"/>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = a_tags_for_img_tags[0].find('img',class_='poster')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp48QGXaYQLs"
   },
   "source": [
    "As said eariler, our **intention** is to get the **URL** for the **image**. Having examined the tag strucure, we could find out that **the value of attribute `src`** can help us to get the image. But we have to concatenate  it with our `base_url` which is `http://themoviedb.org`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOvEoumXYui1",
    "outputId": "17920df5-affb-4a1b-e318-8c9fa399426d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg\n"
     ]
    }
   ],
   "source": [
    "imageURL = base_url + a_tags_for_img_tags[0].find('img',class_='poster')['src']\n",
    "print(imageURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "runCvUPdZYyJ"
   },
   "source": [
    "So far, we have successfully captured data from one tag at one time, whereas we have 20 tags. On the other hand, we have to consider one situation where the site did not have the image for a tv program. I developed if/else statements to avoid this situation. Anyway, let's wrap this up into a function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGhEp2bWZYrE"
   },
   "outputs": [],
   "source": [
    "# To get one img tag\n",
    "def parse_image(img_tag):\n",
    "    \"\"\"To get one <img> tag value\"\"\"\n",
    "    if img_tag != None:\n",
    "      img_url ={'image_url':'https://www.themoviedb.org'+ img_tag['src']}\n",
    "    else:\n",
    "      img_url = {'image_url':\"No photo\"}\n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gBig8zkKpmx"
   },
   "source": [
    "Let's test the function we just wrote to see if we can get a list of 20 image URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3qg9Di7hXQM",
    "outputId": "ddfd0279-2b08-4432-f099-a274bc46ecd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of images on the page is 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/ooBGRQBdbGzBxAVfExiO8r7kloA.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/mS5SLxMYcKfUxA0utBSR5MOAWWr.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/hkiNJTUqgltJvqEpFP1QpzuujO2.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/gKG5QGz5Ngf8fgWpBsWtlg5L2SF.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/qSEKyf0fWhrCEQ3LTwLqe41eSvR.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/d4vPg3QsTJJh6C5MHARTb5CyqOu.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/iehrrb9CmYCiV1hXp5pdGQQmGNe.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/MoEKaPFHABtA1xKoOteirGaHl1.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/wKTAz8fkoXJoHqPpi4ArAUGDtco.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/aESxB2HblKlDzma39xVefa20pbW.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/17QQBTahkRE23bxJ4PmQ7tjMjyX.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/v6Xmj8Fy7ZruVTz3y2Po7O0TQh4.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/xjyEpcuDbB1jy0ehNQMBiO8KOdr.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/f496cm9enuEsZkSPzCwnTESEK5s.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/wu4Vi93RxaE1kkFM5ClmcVzkIby.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/uTSLeQTeHevt4fplegmQ6bOnE0Z.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/3KCAZaKHmoMIN9dHutqaMtubQqD.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/n0ia3oDpfgS3VTx9ZJrwBHihS4a.jpg',\n",
       " 'https://themoviedb.org/t/p/w220_and_h330_face/OlPR1kctwXzSUJQkZINDDhNlHV.jpg']"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get all a tags within the class='image'\n",
    "base_url = 'https://themoviedb.org'\n",
    "a_tags_for_img_tags = page_wrapper.find_all('a', class_='image')\n",
    "img_tags = [a_tag_for_img.find('img',class_='poster') for a_tag_for_img in a_tags_for_img_tags]\n",
    "images_list = [base_url+img_tag['src'] for img_tag in img_tags ]\n",
    "print(\"the total number of images on the page is {}\".format(len(images_list)))\n",
    "images_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2bSfIPxaBzs"
   },
   "source": [
    "### 5. Detailed page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlevIGZ0aMBr"
   },
   "source": [
    "For the detail page, I've tried many methods to figure out how to get this URL. It turns out each TV program has its **unique identifier**. Each identifier actually lives within **`<a>` tags**. It's reasonable to see how it works. Because vistors usually click on images to find out more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fei_JxYbFix"
   },
   "source": [
    "The attribute `href` stands for hyperlink reference and usually comes with `<a>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "JB1PtvSeaLwo",
    "outputId": "eae5b5e4-cf7c-4160-b9da-1158d3effd98"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/tv/79242'"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the value of <a href=XXX>\n",
    "one_a_href= a_tags_for_img_tags[0]['href']\n",
    "one_a_href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9VMj_guLS7w"
   },
   "source": [
    "You can concatenate them with `base_url`. The link can direct you to the specif detailed page about the TV program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DDZ5PYcbowD",
    "outputId": "f1351023-bf46-4b4b-d3bb-c4846ea405c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://themoviedb.org/tv/79242\n"
     ]
    }
   ],
   "source": [
    "#To concatate base url with the attribute href of a tags\n",
    "details_page = base_url + content_tags[0].find('a')['href']\n",
    "print(details_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYr8F-uOiZk3",
    "outputId": "8c4928d9-7ef2-4854-8822-4f93ff244341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of user_score on the page is 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://themoviedb.org/tv/79242',\n",
       " 'http://themoviedb.org/tv/1418',\n",
       " 'http://themoviedb.org/tv/31132',\n",
       " 'http://themoviedb.org/tv/92396',\n",
       " 'http://themoviedb.org/tv/1412',\n",
       " 'http://themoviedb.org/tv/105009',\n",
       " 'http://themoviedb.org/tv/72637',\n",
       " 'http://themoviedb.org/tv/12637',\n",
       " 'http://themoviedb.org/tv/71446',\n",
       " 'http://themoviedb.org/tv/80350',\n",
       " 'http://themoviedb.org/tv/71728',\n",
       " 'http://themoviedb.org/tv/12926',\n",
       " 'http://themoviedb.org/tv/80828',\n",
       " 'http://themoviedb.org/tv/89247',\n",
       " 'http://themoviedb.org/tv/1668',\n",
       " 'http://themoviedb.org/tv/9027',\n",
       " 'http://themoviedb.org/tv/71789',\n",
       " 'http://themoviedb.org/tv/46639',\n",
       " 'http://themoviedb.org/tv/65555',\n",
       " 'http://themoviedb.org/tv/58841']"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To ge a list of details page on the same pages\n",
    "base_url = 'http://themoviedb.org'\n",
    "content_tags = page_wrapper.find_all('div',class_=\"content\")\n",
    "a_tags_for_details = [content_tag.find('a') for content_tag in content_tags]\n",
    "detail_pages = [base_url+a_tag_for_details['href'] for a_tag_for_details in a_tags_for_details ]\n",
    "print(\"the total number of user_score on the page is {}\".format(len(detail_pages)))\n",
    "detail_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0oOs96Kchdy"
   },
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we know how to extract information from the HTMl document using Beautiful Soup library. \n",
    "\n",
    "- We have learned HTML basics\n",
    "\n",
    "- We have analyzed the HTML structure and contents\n",
    "\n",
    "- We have successfully extracted information about movie title, released date, image URL, details URL, user scores.\n",
    "\n",
    "- We have written some helper functions such as `parse_images()`, `convert_date()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTabCIWFdW3t"
   },
   "source": [
    "### Let's wrap them up into a function\n",
    "\n",
    "In this code cell, you will see how I combine the functions we have written so far into one single block. It's helpful to be progressive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMy-8kqTdZ1k"
   },
   "outputs": [],
   "source": [
    "def parse_content(content_tag):\n",
    "    # a tag contains title name\n",
    "      a_tag = content_tag.find('a')\n",
    "    # movie title name\n",
    "      movie_title = a_tag.text\n",
    "    # p tag contains the released date\n",
    "      p_tag = content_tag.find('p')\n",
    "    # released date\n",
    "      rel_date=convert_date(p_tag.text)\n",
    "    # detail page\n",
    "      det_url ='https://www.themoviedb.org' + a_tag['href']\n",
    "    # span tag containing user score\n",
    "      span_tag = content_tag.find('span')\n",
    "    # user score\n",
    "      user_score = span_tag['class'][-1][-2:] + '%'\n",
    "\n",
    "    # return a dictionary\n",
    "      return {\n",
    "          'movie_title': movie_title,\n",
    "          'released_date': rel_date,\n",
    "          'user_score': user_score,\n",
    "          'detail_url': det_url\n",
    "          } \n",
    "\n",
    "def parse_image(img_tag):\n",
    "    \"\"\"To get one <img> value\"\"\"\n",
    "    if img_tag != None:\n",
    "      img_url ={'image_url':'https://www.themoviedb.org'+ img_tag['src']}\n",
    "    else:\n",
    "      img_url = {'image_url':\"Not_found\"}\n",
    "    #img_url ={'image_url':'https://www.themoviedb.org'+ img_tag['src']}\n",
    "    return img_url\n",
    "\n",
    "# Write a helper function to clean date data\n",
    "def convert_date(p_tag):\n",
    "  if p_tag =='':\n",
    "    return \"Not Found\"\n",
    "  elif p_tag is None:\n",
    "    return \"Not Found\"\n",
    "  else:\n",
    "    date = dt.datetime.strptime(p_tag, \"%b %d, %Y\")\n",
    "    released_date = '{}-{}-{}'.format(date.year,date.month,date.day)\n",
    "    return released_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JulI-Lg6ZPMP"
   },
   "source": [
    "# Complie extracted information into Python list and dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL2gWqege1ol"
   },
   "source": [
    "## Dictionary Concatenation \n",
    "So far we have successfully capture information from one single tag and what we get is dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbeckR0rZQg3",
    "outputId": "b68bb691-bdf8-4b2c-82d6-f060bd7e9f7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail_url': 'https://www.themoviedb.org/tv/79242',\n",
       " 'movie_title': 'Chilling Adventures of Sabrina',\n",
       " 'released_date': '2018-10-26',\n",
       " 'user_score': '84%'}"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_tag = content_tags[0]\n",
    "parse_content(content_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ul26JsENZZpF",
    "outputId": "771c5ef4-d4c7-43df-9ff8-b631ed138d04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg'}"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tag = img_tags[0]\n",
    "parse_image(img_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S10-0L5OjnvL"
   },
   "source": [
    "**How can we concatenate with these 2 dictionary?**\n",
    "\n",
    "\n",
    "Right, we can use `dict.update()` to achieve what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KM_mNSBZZmM",
    "outputId": "1b41e4e7-e9b5-4094-d58b-c659035a736f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail_url': 'https://www.themoviedb.org/tv/79242',\n",
       " 'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg',\n",
       " 'movie_title': 'Chilling Adventures of Sabrina',\n",
       " 'released_date': '2018-10-26',\n",
       " 'user_score': '84%'}"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To assign a variable d1 to the outcome of the function\n",
    "d1 = parse_content(content_tag)\n",
    "#To assign another variable to d2\n",
    "d2 = parse_image(img_tag)\n",
    "d3=dict(d1)\n",
    "d3.update(d2)\n",
    "d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXZuddaqlz17"
   },
   "source": [
    "Let's wrap this up into a helper function. Whenever we pass `content_tags`, `img_tags` which is what `parse_content()` and `parse_image()` return, we can get a list of dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPSPfSIVlwP5"
   },
   "outputs": [],
   "source": [
    "def all_content(content_tags,img_tags):\n",
    "    all_content = []\n",
    "    for i in range(20):\n",
    "      d1 = parse_content(content_tags[i])\n",
    "      d2 = parse_image(img_tags[i])\n",
    "      d4 = dict(d1)\n",
    "      d4.update(d2)\n",
    "      all_content.append(d4)\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdy_Vc7AMJ0s"
   },
   "source": [
    "Let's test this function out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvTJp3sNML3G",
    "outputId": "bbdb14f4-d617-48ee-f859-8bbbbb80ab59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'detail_url': 'https://www.themoviedb.org/tv/79242',\n",
       "  'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg',\n",
       "  'movie_title': 'Chilling Adventures of Sabrina',\n",
       "  'released_date': '2018-10-26',\n",
       "  'user_score': '84%'},\n",
       " {'detail_url': 'https://www.themoviedb.org/tv/1418',\n",
       "  'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/ooBGRQBdbGzBxAVfExiO8r7kloA.jpg',\n",
       "  'movie_title': 'The Big Bang Theory',\n",
       "  'released_date': '2007-9-24',\n",
       "  'user_score': '77%'},\n",
       " {'detail_url': 'https://www.themoviedb.org/tv/31132',\n",
       "  'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/mS5SLxMYcKfUxA0utBSR5MOAWWr.jpg',\n",
       "  'movie_title': 'Regular Show',\n",
       "  'released_date': '2010-9-6',\n",
       "  'user_score': '87%'},\n",
       " {'detail_url': 'https://www.themoviedb.org/tv/92396',\n",
       "  'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/hkiNJTUqgltJvqEpFP1QpzuujO2.jpg',\n",
       "  'movie_title': 'Lady, la vendedora de rosas',\n",
       "  'released_date': 'Not Found',\n",
       "  'user_score': '74%'},\n",
       " {'detail_url': 'https://www.themoviedb.org/tv/1412',\n",
       "  'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/gKG5QGz5Ngf8fgWpBsWtlg5L2SF.jpg',\n",
       "  'movie_title': 'Arrow',\n",
       "  'released_date': '2012-10-10',\n",
       "  'user_score': '66%'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_content_list = all_content(content_tags,img_tags)\n",
    "print(type(all_content_list))\n",
    "all_content_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wRJcA61MYkt"
   },
   "source": [
    "Great. we're getting close to what we wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7YgI7deaYJ2"
   },
   "source": [
    "Until now, we can write another function to capture the output from the function `get_page()` combining with the function `all_content()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xqo3eQsXaXbl"
   },
   "outputs": [],
   "source": [
    "def get_one_page(doc):\n",
    "    \"\"\"Parse all content on one page given a BeautifulSoup object\"\"\"\n",
    "      # Get the div containing all contents\n",
    "    page_wrappers = doc.find_all('div',class_='page_wrapper')\n",
    "      # Get the content tags containing title, released date, user score\n",
    "    content_tags = page_wrappers[0].find_all('div',class_=\"content\")\n",
    "      # Get a tag containing <img>\n",
    "    a_tags = page_wrappers[0].find_all('a', class_='image')\n",
    "      # Get a list of all <img>s\n",
    "    img_tags = [tag.find('img',class_='poster') for tag in a_tags]\n",
    "      # Put them all into a list of dictionary togeter \n",
    "    all_page_contents = all_content(content_tags,img_tags)\n",
    "    return all_page_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d5xEwN3kSr2"
   },
   "source": [
    "# Write information to CSV files\n",
    "\n",
    "We aim to get the CSV file. We can try to write a simple code to get this outcome into csv file.\n",
    "\n",
    "Before we do that, let's test out how we can write one item into a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeNTtsmCNd9o"
   },
   "source": [
    "You can see `d3` just have one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5lPy5y3NcO0",
    "outputId": "f44974b0-4e30-4231-f8af-3a0441c62d85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail_url': 'https://www.themoviedb.org/tv/79242',\n",
       " 'image_url': 'https://www.themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg',\n",
       " 'movie_title': 'Chilling Adventures of Sabrina',\n",
       " 'released_date': '2018-10-26',\n",
       " 'user_score': '84%'}"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GEcm31sN1xz"
   },
   "source": [
    "Let's use `open` statement to create a text file called `test.csv` and we can save it in Google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rqfO8DZZZjs"
   },
   "outputs": [],
   "source": [
    "with open(\"test.csv\",'w') as f:\n",
    "        # Write the headers or filed names in the first line\n",
    "        headers =list(d3.keys())\n",
    "        f.write(','.join(headers)+'\\n')\n",
    "\n",
    "        # Write one item per line\n",
    "      \n",
    "        values = []\n",
    "        for header in headers:\n",
    "                values.append(str(d3.get(header,\"\")))\n",
    "        f.write(','.join(values)+\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EREd-NMENCjs"
   },
   "source": [
    "We can use `open` statement again to see if we successfully created `test.csv` and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgIBra7TNLHK",
    "outputId": "83998132-9423-47cd-b50e-b840ec2f7d98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_title,released_date,user_score,detail_url,image_url\n",
      "Chilling Adventures of Sabrina,2018-10-26,84%,https://www.themoviedb.org/tv/79242,https://www.themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('test.csv','r') as f:\n",
    "  f=f.read()\n",
    "  print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amDeePfwlmfv"
   },
   "source": [
    "Let's write a helper function for writng and reading a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rvlVqAIlkm_"
   },
   "outputs": [],
   "source": [
    "def write_csv(items, path):\n",
    "    # Open the file in write mode\n",
    "    with open(path, 'w') as f:\n",
    "        # Return if there's nothing to write\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        \n",
    "        # Write the headers in the first line\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        \n",
    "        # Write one item per line\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            f.write(','.join(values) + \"\\n\")\n",
    "\n",
    "def read_csv(path):\n",
    "    with open(path,'r') as f:\n",
    "      f=f.read()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNyaCMmyN_Gd"
   },
   "source": [
    "Then we can write a for loop to loop a list of `all_content_list` into the file `another_test.csv` using this function we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psWsd6UFOEm_"
   },
   "outputs": [],
   "source": [
    "write_csv(all_content_list,'another_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "kQhZzvatOmre",
    "outputId": "0a784be7-a42b-4fec-9716-d188d111015a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'movie_title,released_date,user_score,detail_url,image_url\\nChilling Adventures of Sabrina,2018-10-26,84%,https://www.themoviedb.org/tv/79242,https://www.themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg\\nThe Big Bang Theory,2007-9-24,77%,https://www.themoviedb.org/tv/1418,https://www.themoviedb.org/t/p/w220_and_h330_face/ooBGRQBdbGzBxAVfExiO8r7kloA.jpg\\nRegular Show,2010-9-6,87%,https://www.themoviedb.org/tv/31132,https://www.themoviedb.org/t/p/w220_and_h330_face/mS5SLxMYcKfUxA0utBSR5MOAWWr.jpg\\nLady, la vendedora de rosas,Not Found,74%,https://www.themoviedb.org/tv/92396,https://www.themoviedb.org/t/p/w220_and_h330_face/hkiNJTUqgltJvqEpFP1QpzuujO2.jpg\\nArrow,2012-10-10,66%,https://www.themoviedb.org/tv/1412,https://www.themoviedb.org/t/p/w220_and_h330_face/gKG5QGz5Ngf8fgWpBsWtlg5L2SF.jpg\\nTokyo Revengers,2021-4-11,90%,https://www.themoviedb.org/tv/105009,https://www.themoviedb.org/t/p/w220_and_h330_face/qSEKyf0fWhrCEQ3LTwLqe41eSvR.jpg\\nOnce,2017-6-19,88%,https://www.themoviedb.org/tv/72637,https://www.themoviedb.org/t/p/w220_and_h330_face/d4vPg3QsTJJh6C5MHARTb5CyqOu.jpg\\nRebelde,2004-10-4,85%,https://www.themoviedb.org/tv/12637,https://www.themoviedb.org/t/p/w220_and_h330_face/iehrrb9CmYCiV1hXp5pdGQQmGNe.jpg\\nMoney Heist,2017-5-2,83%,https://www.themoviedb.org/tv/71446,https://www.themoviedb.org/t/p/w220_and_h330_face/MoEKaPFHABtA1xKoOteirGaHl1.jpg\\nNew Amsterdam,2018-9-25,84%,https://www.themoviedb.org/tv/80350,https://www.themoviedb.org/t/p/w220_and_h330_face/wKTAz8fkoXJoHqPpi4ArAUGDtco.jpg\\nYoung Sheldon,2017-9-25,80%,https://www.themoviedb.org/tv/71728,https://www.themoviedb.org/t/p/w220_and_h330_face/aESxB2HblKlDzma39xVefa20pbW.jpg\\nTeresa,2010-8-2,75%,https://www.themoviedb.org/tv/12926,https://www.themoviedb.org/t/p/w220_and_h330_face/17QQBTahkRE23bxJ4PmQ7tjMjyX.jpg\\nThe Nevers,2021-4-11,87%,https://www.themoviedb.org/tv/80828,https://www.themoviedb.org/t/p/w220_and_h330_face/v6Xmj8Fy7ZruVTz3y2Po7O0TQh4.jpg\\nBatwoman,2019-10-6,73%,https://www.themoviedb.org/tv/89247,https://www.themoviedb.org/t/p/w220_and_h330_face/xjyEpcuDbB1jy0ehNQMBiO8KOdr.jpg\\nFriends,1994-9-22,84%,https://www.themoviedb.org/tv/1668,https://www.themoviedb.org/t/p/w220_and_h330_face/f496cm9enuEsZkSPzCwnTESEK5s.jpg\\nRebelde Way,2002-5-27,84%,https://www.themoviedb.org/tv/9027,https://www.themoviedb.org/t/p/w220_and_h330_face/wu4Vi93RxaE1kkFM5ClmcVzkIby.jpg\\nSEAL Team,2017-9-27,78%,https://www.themoviedb.org/tv/71789,https://www.themoviedb.org/t/p/w220_and_h330_face/uTSLeQTeHevt4fplegmQ6bOnE0Z.jpg\\nAmerican Gods,2017-4-30,71%,https://www.themoviedb.org/tv/46639,https://www.themoviedb.org/t/p/w220_and_h330_face/3KCAZaKHmoMIN9dHutqaMtubQqD.jpg\\nEndless Love,2015-10-14,77%,https://www.themoviedb.org/tv/65555,https://www.themoviedb.org/t/p/w220_and_h330_face/n0ia3oDpfgS3VTx9ZJrwBHihS4a.jpg\\nChicago P.D.,2014-1-8,84%,https://www.themoviedb.org/tv/58841,https://www.themoviedb.org/t/p/w220_and_h330_face/OlPR1kctwXzSUJQkZINDDhNlHV.jpg\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_csv('another_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "bB7Qkc2UOyUO",
    "outputId": "2e945c5b-5294-40bc-aa77-501cb375def9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "[jovian] Capturing environment..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/shenghongzhong/210424-project001-web-scraping-tmbd\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/shenghongzhong/210424-project001-web-scraping-tmbd'"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9ppOXUkZB34"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqLqUDTKl_Mh"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have written functions such as \n",
    "\n",
    "- `get_page()` for an HTTP request and returning with Beautiful Soup\n",
    "\n",
    "- `parse_content()` for parsing all contents such as movie title, released date, detailed page, user scores on the web page.\n",
    "\n",
    "- `convert_date()` for cleaning date data into a nice format\n",
    "\n",
    "- `parse_image()` for parsing one image tag into a dictionary\n",
    "\n",
    "- `all_contents()` for concatenating two outputs from `parse_content()` and `parse_image()` into a list that comprises of elements in the data structure of dictionary \n",
    "\n",
    "- `get_one_page()` is a nice function we designed to combine with `all_content()` and `get_page()` as well. It returns a list of dictionary\n",
    "\n",
    "- `write_csv()` and `read_csv()` for write a list of dictionary as an output of `all_contents()`\n",
    "\n",
    "\n",
    "\n",
    "Now, every piece of the scraper has assembled together. We need to write a function to help us to get one single page of data and output a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nYUIo9Wms9u"
   },
   "source": [
    "# One page web scraper\n",
    "\n",
    "So far, we can combine all pieces of scraper compontents into a single function to get one page of data if we specify a page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btw1fgrGl-6e"
   },
   "outputs": [],
   "source": [
    "# To get one page of data\n",
    "def web_scraper(base_url=None,page_number=None,path=None,get_content=False):\n",
    "  \"\"\"Get the content for no. of page and write them to a CSV file\"\"\"\n",
    "    #if path isn't specified, the default goes with the current time\n",
    "  if path is None:\n",
    "        from datetime import datetime\n",
    "        path = datetime.now().strftime(\"%Y-%b-%d %H:%M:%S\") + '.csv'\n",
    "    # if page isn't specified, the default of page number is 1\n",
    "  if base_url is None:\n",
    "        base_url = 'https://www.themoviedb.org'\n",
    "  #if page_number is None or page_number = 1 or page_numbr = 0:\n",
    "  if page_number is None:\n",
    "        page_number = 1\n",
    "  page_doc = get_page(page_number)\n",
    "  page_content = get_one_page(page_doc)\n",
    "  #print(page_content)\n",
    "  if get_content:\n",
    "        return page_content\n",
    "  else:\n",
    "        write_csv(page_content,path)\n",
    "\n",
    "  \n",
    "  print('You have successfully scraped data at the page {}, written to file {}'.format(page_number, path))\n",
    "  return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr_WYl1S4XEg"
   },
   "source": [
    "Import all libraries and modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FO7uYpQmhT_"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8z0atmiP4J2n",
    "outputId": "71f41fa3-6de0-4c06-9cc5-fde63739ab3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully scraped data at the page 5, written to file 2021-Apr-28 17:01:02.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file = web_scraper(page_number=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6maN2Zf4P7F"
   },
   "source": [
    "Using `with` statement to check the data we capture from the page 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3koii7rMzXoF",
    "outputId": "9571186a-0dac-4fbd-9539-b9a0b287ddab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_title,released_date,user_score,detail_url,image_url\n",
      "Chilling Adventures of Sabrina,2018-10-26,84%,https://www.themoviedb.org/tv/79242,https://www.themoviedb.org/t/p/w220_and_h330_face/yxMpoHO0CXP5o9gB7IfsciilQS4.jpg\n",
      "The Big Bang Theory,2007-9-24,77%,https://www.themoviedb.org/tv/1418,https://www.themoviedb.org/t/p/w220_and_h330_face/ooBGRQBdbGzBxAVfExiO8r7kloA.jpg\n",
      "Regular Show,2010-9-6,87%,https://www.themoviedb.org/tv/31132,https://www.themoviedb.org/t/p/w220_and_h330_face/mS5SLxMYcKfUxA0utBSR5MOAWWr.jpg\n",
      "Lady, la vendedora de rosas,Not Found,74%,https://www.themoviedb.org/tv/92396,https://www.themoviedb.org/t/p/w220_and_h330_face/hkiNJTUqgltJvqEpFP1QpzuujO2.jpg\n",
      "Arrow,2012-10-10,66%,https://www.themoviedb.org/tv/1412,https://www.themoviedb.org/t/p/w220_and_h330_face/gKG5QGz5Ngf8fgWpBsWtlg5L2SF.jpg\n",
      "Tokyo Revengers,2021-4-11,90%,https://www.themoviedb.org/tv/105009,https://www.themoviedb.org/t/p/w220_and_h330_face/qSEKyf0fWhrCEQ3LTwLqe41eSvR.jpg\n",
      "Once,2017-6-19,88%,https://www.themoviedb.org/tv/72637,https://www.themoviedb.org/t/p/w220_and_h330_face/d4vPg3QsTJJh6C5MHARTb5CyqOu.jpg\n",
      "Rebelde,2004-10-4,85%,https://www.themoviedb.org/tv/12637,https://www.themoviedb.org/t/p/w220_and_h330_face/iehrrb9CmYCiV1hXp5pdGQQmGNe.jpg\n",
      "Money Heist,2017-5-2,83%,https://www.themoviedb.org/tv/71446,https://www.themoviedb.org/t/p/w220_and_h330_face/MoEKaPFHABtA1xKoOteirGaHl1.jpg\n",
      "New Amsterdam,2018-9-25,84%,https://www.themoviedb.org/tv/80350,https://www.themoviedb.org/t/p/w220_and_h330_face/wKTAz8fkoXJoHqPpi4ArAUGDtco.jpg\n",
      "Young Sheldon,2017-9-25,80%,https://www.themoviedb.org/tv/71728,https://www.themoviedb.org/t/p/w220_and_h330_face/aESxB2HblKlDzma39xVefa20pbW.jpg\n",
      "Teresa,2010-8-2,75%,https://www.themoviedb.org/tv/12926,https://www.themoviedb.org/t/p/w220_and_h330_face/17QQBTahkRE23bxJ4PmQ7tjMjyX.jpg\n",
      "The Nevers,2021-4-11,87%,https://www.themoviedb.org/tv/80828,https://www.themoviedb.org/t/p/w220_and_h330_face/v6Xmj8Fy7ZruVTz3y2Po7O0TQh4.jpg\n",
      "Batwoman,2019-10-6,73%,https://www.themoviedb.org/tv/89247,https://www.themoviedb.org/t/p/w220_and_h330_face/xjyEpcuDbB1jy0ehNQMBiO8KOdr.jpg\n",
      "Friends,1994-9-22,84%,https://www.themoviedb.org/tv/1668,https://www.themoviedb.org/t/p/w220_and_h330_face/f496cm9enuEsZkSPzCwnTESEK5s.jpg\n",
      "Rebelde Way,2002-5-27,84%,https://www.themoviedb.org/tv/9027,https://www.themoviedb.org/t/p/w220_and_h330_face/wu4Vi93RxaE1kkFM5ClmcVzkIby.jpg\n",
      "SEAL Team,2017-9-27,78%,https://www.themoviedb.org/tv/71789,https://www.themoviedb.org/t/p/w220_and_h330_face/uTSLeQTeHevt4fplegmQ6bOnE0Z.jpg\n",
      "American Gods,2017-4-30,71%,https://www.themoviedb.org/tv/46639,https://www.themoviedb.org/t/p/w220_and_h330_face/3KCAZaKHmoMIN9dHutqaMtubQqD.jpg\n",
      "Endless Love,2015-10-14,77%,https://www.themoviedb.org/tv/65555,https://www.themoviedb.org/t/p/w220_and_h330_face/n0ia3oDpfgS3VTx9ZJrwBHihS4a.jpg\n",
      "Chicago P.D.,2014-1-8,84%,https://www.themoviedb.org/tv/58841,https://www.themoviedb.org/t/p/w220_and_h330_face/OlPR1kctwXzSUJQkZINDDhNlHV.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(csv_file,'r') as f:\n",
    "  f=f.read()\n",
    "  print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnGTxgFgZaIH"
   },
   "source": [
    "# Extract and combine data from multiple pages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlNQrfFl4cjd"
   },
   "source": [
    "Since we can get one page of data, we can simply write another function on top of the helper function. I named it `ultra_scraper()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9HrhdvsZawF"
   },
   "outputs": [],
   "source": [
    "def ultra_scraper(base_url=None,page_number=None,path=None,get_content=False):\n",
    "  \"\"\"Get the content for no. of page and write them to a CSV file\"\"\"\n",
    "    #if path isn't specified, the default goes with the current time\n",
    "  if path is None:\n",
    "        from datetime import datetime\n",
    "        path = datetime.now().strftime(\"%Y-%b-%d %H:%M:%S\") + '.csv'\n",
    "    # if page isn't specified, the default of page number is 1\n",
    "  if base_url is None:\n",
    "        base_url = 'https://www.themoviedb.org'\n",
    "  #if page_number is None or page_number = 1 or page_numbr = 0:\n",
    "  if page_number is None:\n",
    "        page_number = 1\n",
    "  page_contents = []\n",
    "  for i in range (1,page_number+1):\n",
    "      time.sleep(1)\n",
    "      print('Downloading page {}...Please patiently wait...'.format(str(i)))\n",
    "      page_contents+= web_scraper(base_url=base_url,page_number=i,path=path,get_content=True)\n",
    "  time.sleep(1)\n",
    "  print('Downloading is done. Thank you for your patience!')\n",
    "  import pandas\n",
    "  dataframe = pandas.DataFrame(page_contents)\n",
    "  dataframe.to_csv(path, index=None)\n",
    "  print('You have successfully scraped data at the {} pages, written to file {}'.format(page_number, path))\n",
    "  return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMwL7FnyT5sZ"
   },
   "source": [
    "Using `ultra_scraper()` to extrat data from the 20 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SnpCcH6Zdhk",
    "outputId": "68313cc7-4793-439c-861b-2b31d550fa0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 1...Please patiently wait...\n",
      "Downloading page 2...Please patiently wait...\n",
      "Downloading page 3...Please patiently wait...\n",
      "Downloading page 4...Please patiently wait...\n",
      "Downloading page 5...Please patiently wait...\n",
      "Downloading page 6...Please patiently wait...\n",
      "Downloading page 7...Please patiently wait...\n",
      "Downloading page 8...Please patiently wait...\n",
      "Downloading page 9...Please patiently wait...\n",
      "Downloading page 10...Please patiently wait...\n",
      "Downloading page 11...Please patiently wait...\n",
      "Downloading page 12...Please patiently wait...\n",
      "Downloading page 13...Please patiently wait...\n",
      "Downloading page 14...Please patiently wait...\n",
      "Downloading page 15...Please patiently wait...\n",
      "Downloading page 16...Please patiently wait...\n",
      "Downloading page 17...Please patiently wait...\n",
      "Downloading page 18...Please patiently wait...\n",
      "Downloading page 19...Please patiently wait...\n",
      "Downloading page 20...Please patiently wait...\n",
      "...\n",
      "Downloading is done. Thank you for your patience!\n",
      "You have successfully scraped data at the 20 pages, written to file 2021-Apr-28 17:03:34.csv\n"
     ]
    }
   ],
   "source": [
    "data = ultra_scraper(page_number=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tbP1x8XPODES",
    "outputId": "92376095-feea-446e-85d4-06491518b5ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2021-Apr-28 17:03:34.csv'"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkgNZig-TxLs"
   },
   "source": [
    "Check if extracted data is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "vK5-z5Iw7tC_",
    "outputId": "e84b78e2-20c2-43ff-9beb-2266b1f73ead"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>released_date</th>\n",
       "      <th>user_score</th>\n",
       "      <th>detail_url</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Falcon and the Winter Soldier</td>\n",
       "      <td>2021-3-19</td>\n",
       "      <td>79%</td>\n",
       "      <td>https://www.themoviedb.org/tv/88396</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Good Doctor</td>\n",
       "      <td>2017-9-25</td>\n",
       "      <td>86%</td>\n",
       "      <td>https://www.themoviedb.org/tv/71712</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luis Miguel: The Series</td>\n",
       "      <td>2018-4-22</td>\n",
       "      <td>81%</td>\n",
       "      <td>https://www.themoviedb.org/tv/79008</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Flash</td>\n",
       "      <td>2014-10-7</td>\n",
       "      <td>77%</td>\n",
       "      <td>https://www.themoviedb.org/tv/60735</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Van Helsing</td>\n",
       "      <td>2016-9-23</td>\n",
       "      <td>69%</td>\n",
       "      <td>https://www.themoviedb.org/tv/65820</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>NOVA</td>\n",
       "      <td>1974-3-3</td>\n",
       "      <td>71%</td>\n",
       "      <td>https://www.themoviedb.org/tv/3562</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Demon Slayer Academy Valentine Chapter</td>\n",
       "      <td>2021-2-14</td>\n",
       "      <td>NR%</td>\n",
       "      <td>https://www.themoviedb.org/tv/118405</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>ZDF-Mittagsmagazin</td>\n",
       "      <td>1989-10-2</td>\n",
       "      <td>NR%</td>\n",
       "      <td>https://www.themoviedb.org/tv/105002</td>\n",
       "      <td>Not_found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Batman: The Animated Series</td>\n",
       "      <td>1992-9-5</td>\n",
       "      <td>83%</td>\n",
       "      <td>https://www.themoviedb.org/tv/2098</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Merlin</td>\n",
       "      <td>2008-9-20</td>\n",
       "      <td>77%</td>\n",
       "      <td>https://www.themoviedb.org/tv/7225</td>\n",
       "      <td>https://www.themoviedb.org/t/p/w220_and_h330_f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                movie_title  ...                                          image_url\n",
       "0         The Falcon and the Winter Soldier  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "1                           The Good Doctor  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "2                   Luis Miguel: The Series  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "3                                 The Flash  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "4                               Van Helsing  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "..                                      ...  ...                                                ...\n",
       "395                                    NOVA  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "396  Demon Slayer Academy Valentine Chapter  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "397                      ZDF-Mittagsmagazin  ...                                          Not_found\n",
       "398             Batman: The Animated Series  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "399                                  Merlin  ...  https://www.themoviedb.org/t/p/w220_and_h330_f...\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ7BxhnXD6Qv"
   },
   "source": [
    "# Put everything into one code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCWTSgsICIA8"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "\n",
    "def ultra_scraper(base_url=None,page_number=None,path=None,get_content=False):\n",
    "  \"\"\"Get the content for no. of page and write them to a CSV file\"\"\"\n",
    "    #if path isn't specified, the default goes with the current time\n",
    "  if path is None:\n",
    "        from datetime import datetime\n",
    "        path = datetime.now().strftime(\"%Y-%b-%d %H:%M:%S\") + '.csv'\n",
    "    # if page isn't specified, the default of page number is 1\n",
    "  if base_url is None:\n",
    "        base_url = 'https://www.themoviedb.org'\n",
    "  #if page_number is None or page_number = 1 or page_numbr = 0:\n",
    "  if page_number is None:\n",
    "        page_number = 1\n",
    "  page_contents = []\n",
    "  for i in range (1,page_number+1):\n",
    "      time.sleep(1)\n",
    "      print('Downloading page {}...Please patiently wait...'.format(str(i)))\n",
    "      page_contents+= web_scraper(base_url=base_url,page_number=i,path=path,get_content=True)\n",
    "  time.sleep(1)\n",
    "  print('Downloading is done. Thank you for your patience!')\n",
    "  import pandas\n",
    "  dataframe = pandas.DataFrame(page_contents)\n",
    "  dataframe.to_csv(path, index=None)\n",
    "  print('You have successfully scraped data at the {} pages, written to file {}'.format(page_number, path))\n",
    "  return path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To get one page of data\n",
    "def web_scraper(base_url=None,page_number=None,path=None,get_content=False):\n",
    "  \"\"\"Get the content for no. of page and write them to a CSV file\"\"\"\n",
    "    #if path isn't specified, the default goes with the current time\n",
    "  if path is None:\n",
    "        from datetime import datetime\n",
    "        path = datetime.now().strftime(\"%Y-%b-%d %H:%M:%S\") + '.csv'\n",
    "    # if page isn't specified, the default of page number is 1\n",
    "  if base_url is None:\n",
    "        base_url = 'https://www.themoviedb.org'\n",
    "  #if page_number is None or page_number = 1 or page_numbr = 0:\n",
    "  if page_number is None:\n",
    "        page_number = 1\n",
    "  page_doc = get_page(page_number)\n",
    "  page_content = get_one_page(page_doc)\n",
    "  #print(page_content)\n",
    "  if get_content:\n",
    "        return page_content\n",
    "  else:\n",
    "        write_csv(page_content,path)\n",
    "\n",
    "  \n",
    "  print('You have successfully scraped data at the page {}, written to file {}'.format(page_number, path))\n",
    "  return path\n",
    "\n",
    "# request and store page into a BeautifulSoup object\n",
    "def get_page(page_number):\n",
    "    \"\"\"Get the number of web page containing all the content for TV shows and retun a BeautifulSoup document\"\"\"\n",
    "    page_url = 'https://www.themoviedb.org/tv' + '?page=' + str(page_number)\n",
    "    response = requests.get(page_url)\n",
    "    #check the status\n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception ('Failed to fetch web page' + page_url)\n",
    "    return BeautifulSoup(response.text)\n",
    "\n",
    "def get_one_page(doc):\n",
    "    \"\"\"Parse all content on one page given a BeautifulSoup object\"\"\"\n",
    "      # Get the div containing all contents\n",
    "    page_wrappers = doc.find_all('div',class_='page_wrapper')\n",
    "      # Get the content tags containing title, released date, user score\n",
    "    content_tags = page_wrappers[0].find_all('div',class_=\"content\")\n",
    "      # Get a tag containing <img>\n",
    "    a_tags = page_wrappers[0].find_all('a', class_='image')\n",
    "      # Get a list of all <img>s\n",
    "    img_tags = [tag.find('img',class_='poster') for tag in a_tags]\n",
    "      # Put them all into a list of dictionary togeter \n",
    "    all_page_contents = all_content(content_tags,img_tags)\n",
    "    return all_page_contents\n",
    "      \n",
    "\n",
    "def parse_content(content_tag):\n",
    "    # a tag contains title name\n",
    "      a_tag = content_tag.find('a')\n",
    "    # movie title name\n",
    "      movie_title = a_tag.text\n",
    "    # p tag contains the released date\n",
    "      p_tag = content_tag.find('p').text\n",
    "    # released date\n",
    "      rel_date = convert_date(p_tag)\n",
    "    # detail page\n",
    "      det_url ='https://www.themoviedb.org' + a_tag['href']\n",
    "    # span tag containing user score\n",
    "      span_tag = content_tag.find('span')\n",
    "    # user score\n",
    "      user_score = span_tag['class'][-1][-2:] + '%'\n",
    "\n",
    "    # return a dictionary\n",
    "      return {\n",
    "          'movie_title': movie_title,\n",
    "          'released_date': rel_date,\n",
    "          'user_score': user_score,\n",
    "          'detail_url': det_url\n",
    "          } \n",
    "\n",
    "def convert_date(p_tag):\n",
    "  date = dt.datetime.strptime(p_tag, \"%b %d, %Y\")\n",
    "  return '{}-{}-{}'.format(date.year,date.month,date.day)\n",
    "\n",
    "\n",
    "def parse_image(img_tag):\n",
    "    \"\"\"To get one <img> value\"\"\"\n",
    "    if img_tag != None:\n",
    "      img_url ={'image_url':'https://www.themoviedb.org'+ img_tag['src']}\n",
    "    else:\n",
    "      img_url = {'image_url':\"Not_found\"}\n",
    "    #img_url ={'image_url':'https://www.themoviedb.org'+ img_tag['src']}\n",
    "    return img_url\n",
    "\n",
    "def all_content(content_tags,img_tags):\n",
    "    all_content = []\n",
    "    for i in range(20):\n",
    "      d1 = parse_content(content_tags[i])\n",
    "      d2 = parse_image(img_tags[i])\n",
    "      d4 = dict(d1)\n",
    "      d4.update(d2)\n",
    "      all_content.append(d4)\n",
    "    return all_content\n",
    "\n",
    "def write_csv(items,path):\n",
    "    \"\"\" items is in dictionary type data structure\n",
    "        path is the desired file path \"\"\"\n",
    "    # Open the file in write mode -'w'\n",
    "    with open(path,'w') as f:\n",
    "        # Return if there is nothing to write\n",
    "        if len(items) ==0 :\n",
    "          return None\n",
    "        \n",
    "        # Write the headers or filed names in the first line\n",
    "        headers =list(items[0].keys())\n",
    "        f.write(','.join(headers)+'\\n')\n",
    "\n",
    "        # Write one item per line\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header,\"\")))\n",
    "            f.write(','.join(values)+\"\\n\")\n",
    "\n",
    "# Write a helper function to clean date data\n",
    "def convert_date(p_tag):\n",
    "  if p_tag =='':\n",
    "    return \"Not Found\"\n",
    "  elif p_tag is None:\n",
    "    return \"Not Found\"\n",
    "  else:\n",
    "    date = dt.datetime.strptime(p_tag, \"%b %d, %Y\")\n",
    "    released_date = '{}-{}-{}'.format(date.year,date.month,date.day)\n",
    "    return released_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hZHcxAgEDtE"
   },
   "source": [
    "# The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b17wY-FE10h"
   },
   "source": [
    "It was an interesting project. It reminded me of the time at my previous job as a research associate. I was involved in the innovation project about building a data pipeline from Weibo, a Twitter-like Chinese social media platform. Also, the regular activities I did at previous work are collecting all kinds of data from the Internet. I remembered my boss and CTO were calculating the limit rate and how quickly we can get all data ready. The CTO often mentioned \"farming\". I love this word to describe how we collect data from the digital world.\n",
    "\n",
    "Web scraping is the first step to get real data from the real world. It's absolutely exciting. For the limit of time, I wasn't able to do some analysis. However, good questions are better than meaningless action. I remember there is one day I showed my data visualization work to my boss. He messaged me back, \" That's cool. but what is the insight for our clients?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr3rsln3cxQK"
   },
   "source": [
    "# Future work\n",
    "As for future work, if any of you is interested in it, you can develop my code to get more data such as cast, crew and comments to answer these questions as follows:\n",
    "\n",
    "1. Which year do we have the most TV shows?\n",
    "\n",
    "2. What TV shows do users at TMBd comment on the most? If there wasn't sufficient comments, we can collect comments from another site like Rotten Tomatoes, https://www.rottentomatoes.com/ or Reddit, or Twitter using API if neccessary\n",
    "\n",
    "3. Which factors( the number of black actors, female actors, genres) can determine those users to comment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMSwj42ec2At"
   },
   "source": [
    "#Some ideas about new projects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_uI-GmEdLSX"
   },
   "source": [
    "## Project 1 - Looking for aspiring artists on foundation.app\n",
    "\n",
    "As for my future project, I'm interested in doing something with Bitcoin as I'm a big fan of cryptocurrency. As the writing of this, the NFTs (Non-fungible tokens)is a hit topic. An artist called Beeple sold his digital artwork at the price of 6.6 million dollars. People started to realize this is going to be a big thing. In summary, NFTs could be the Internet of Intellectual Property. However, some started to question if NFTs actually solve the problem. What if a person screenshotted someone's digital artwork? Besides, it's interesting to think of the value of the original work comparing with fake. What's the real difference between Mona Lisa and fake Mona Lisa?\n",
    "\n",
    "My opinion is quite simple and just answer key questions like\n",
    "\n",
    "### 1. Who created?\n",
    "\n",
    "This is because the original Mona Lisa is created by Leonardo da Vinci.\n",
    "\n",
    "### 2. How long are they active in the market?\n",
    "Do you know how long Leonardo da Vinci spent painting Mona Lisa? It took him 16 years to finish.\n",
    "\n",
    "NFTs provides us with a new way to support artists. Wait, How on earth can I know who has the potentials for those upcoming artists?\n",
    "So my idea is to get data from those platforms where artists hang out and sell their artwork. You can simply create multiple profiles for those aspiring artists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJAEYtFEdN9s"
   },
   "source": [
    "## Project 2 - Correlation between inflation&corruption rate and volumes on localbitcoin.com\n",
    "\n",
    "I went to a small gathering for bitcoiners in London 2 weeks ago. It was nice weather in Hydepark. One of the people I met was interested in my skills in data science and we are thinking of getting a project done together. It could be my next project.\n",
    "\n",
    "It'd be interesting to know some factors drive people to trade bitcoins.\n",
    "However, the first step would be always to collect data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8XH5xLedWAX"
   },
   "source": [
    "## Project 3 - Frontline reports for e-commerce business\n",
    "\n",
    "You might know Kickstarter or Indiegogo. What about we can provide some sort of service for e-commerce business owners? For example, once we found something that is similar to their products, they got notification. It could be helpful for them to develop their next product planning.\n",
    "\n",
    "\n",
    "Yet, the first step is to get(scraping) data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osGa-49uHfdS"
   },
   "source": [
    "# References\n",
    "\n",
    "\n",
    "[1] Python offical documentation. https://docs.python.org/3/\n",
    "\n",
    "\n",
    "[2] Requests library. https://pypi.org/project/requests/\n",
    "\n",
    "\n",
    "[3] Beautiful Soup documentation. https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "\n",
    "[4] Aakash N S, Introduction to Web Scraping, 2021. https://jovian.ai/aakashns/python-web-scraping-and-rest-api\n",
    "\n",
    "\n",
    "[5] Salmon. M (2017 , Web Scraping Job Postings from Indeed. https://medium.com/@msalmon00/web-scraping-job-postings-from-indeed-96bd588dcb4b\n",
    "\n",
    "\n",
    "[6] Lazar. D(2020), Scraping Medium with Python& Beautiful Soup. https://medium.com/the-innovation/scraping-medium-with-python-beautiful-soup-3314f898bbf5\n",
    "\n",
    "\n",
    "[7] Arif Ul Islam(Ron), How to Become a Pro with Scraping Youtube Videoes in 3 minutes. https://medium.com/brainstation23/how-to-become-a-pro-with-scraping-youtube-videos-in-3-minutes-a6ac56021961\n",
    "\n",
    "\n",
    "[8] Hoekstra.D(2020), How to Scrape Wikipedia Articles with Python, https://www.freecodecamp.org/news/scraping-wikipedia-articles-with-python/ \n",
    "\n",
    "[9] Macaraeg.R(2020), Web Scraping Yahoo Finance. https://towardsdatascience.com/web-scraping-yahoo-finance-477fe3daa852\n",
    "\n",
    "[10] Mohan.M(2020), Web Scraping Python Tutorial – How to Scrape Data From A Website. https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/ \n",
    "\n",
    "[11] Pandas library documentation. https://pandas.pydata.org/docs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "Def0TcD3QwCm",
    "outputId": "0abe88d4-f651-464c-b465-b7104bde70f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "[jovian] Capturing environment..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/shenghongzhong/210424-project001-web-scraping-tmbd\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/shenghongzhong/210424-project001-web-scraping-tmbd'"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(files=data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Web_scraping_TV_shows_from_TMDB_1.ipynb",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}